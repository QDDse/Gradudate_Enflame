{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45035d09",
   "metadata": {},
   "source": [
    "<font size=4, color=red>**Training_script-v1.0**</font>\n",
    "\n",
    "- [Vision_encoder](#Vision_model)\n",
    "- [Text_encoder](#Text_model)\n",
    "- [Unet_model](#Unet_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458add1",
   "metadata": {},
   "source": [
    "**TODO Lists**  \n",
    "- [ ] Running the whole procedure using random data  \n",
    "- [ ] Baseline  \n",
    "- [ ] Achieve the dataloader  \n",
    "- [ ] Try to train and eval the result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7555ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q --upgrade diffusers transformers accelerate\n",
    "import yaml\n",
    "import math\n",
    "import os\n",
    "import timm\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "import datasets\n",
    "import diffusers\n",
    "import transformers\n",
    "# from diffusers import UNet2DConditionModel\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModel\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin\n",
    "from diffusers.models.cross_attention import AttnProcessor\n",
    "# from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "\n",
    "\n",
    "from model import UNet2DConditionModel\n",
    "\n",
    "from torchinfo import summary\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab43e2a",
   "metadata": {},
   "source": [
    "## Vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49bf656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coca_base',\n",
       " 'coca_roberta-ViT-B-32',\n",
       " 'coca_ViT-B-32',\n",
       " 'coca_ViT-L-14',\n",
       " 'convnext_base',\n",
       " 'convnext_base_w',\n",
       " 'convnext_base_w_320',\n",
       " 'convnext_large',\n",
       " 'convnext_large_d',\n",
       " 'convnext_large_d_320',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'convnext_xlarge',\n",
       " 'convnext_xxlarge',\n",
       " 'convnext_xxlarge_320',\n",
       " 'mt5-base-ViT-B-32',\n",
       " 'mt5-xl-ViT-H-14',\n",
       " 'RN50',\n",
       " 'RN50-quickgelu',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'RN101',\n",
       " 'RN101-quickgelu',\n",
       " 'roberta-ViT-B-32',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'ViT-B-16',\n",
       " 'ViT-B-16-plus',\n",
       " 'ViT-B-16-plus-240',\n",
       " 'ViT-B-32',\n",
       " 'ViT-B-32-plus-256',\n",
       " 'ViT-B-32-quickgelu',\n",
       " 'ViT-bigG-14',\n",
       " 'ViT-e-14',\n",
       " 'ViT-g-14',\n",
       " 'ViT-H-14',\n",
       " 'ViT-H-16',\n",
       " 'ViT-L-14',\n",
       " 'ViT-L-14-280',\n",
       " 'ViT-L-14-336',\n",
       " 'ViT-L-16',\n",
       " 'ViT-L-16-320',\n",
       " 'ViT-M-16',\n",
       " 'ViT-M-16-alt',\n",
       " 'ViT-M-32',\n",
       " 'ViT-M-32-alt',\n",
       " 'ViT-S-16',\n",
       " 'ViT-S-16-alt',\n",
       " 'ViT-S-32',\n",
       " 'ViT-S-32-alt',\n",
       " 'vit_medium_patch16_gap_256',\n",
       " 'vit_relpos_medium_patch16_cls_224',\n",
       " 'xlm-roberta-base-ViT-B-32',\n",
       " 'xlm-roberta-large-ViT-H-14']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_dir = '/root/.cache/clip/'\n",
    "open_clip.list_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/open_clip/pretrained.py:301: UserWarning: /root/.cache/clip/vit_b_32-quickgelu-laion400m_e32-46683a32.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      " 25%|█████████▍                           | 154M/605M [50:25<4:41:40, 26.7kiB/s]"
     ]
    }
   ],
   "source": [
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "91f8611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPVisionModel, CLIPVisionConfig\n",
    "# from transformers import AutoProcessor, CLIPModel, CLIPVisionModelWithProjection\n",
    "\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# vision_config = {\n",
    "#     'hidden_size': 768,\n",
    "#     'intermediate_size': 3072,\n",
    "#     'image_size': 224,\n",
    "#     'patch_size': 32,\n",
    "# }\n",
    "# ## create vision model\n",
    "# model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# pixel_lhs = outputs.last_hidden_state ## [B,S,D]\n",
    "# pooled_output = outputs.pooler_output  # pooled CLS states [B, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d30e6",
   "metadata": {},
   "source": [
    "## Text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e9237d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPTextModel, CLIPTextModelWithProjection\n",
    "# from transformers import AutoProcessor, CLIPModel, AutoTokenizer\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = text_encoder(**inputs) \n",
    "# text_lhs = outputs.last_hidden_state # [B, S, D]\n",
    "# pooled_output = outputs.pooler_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7791b56d",
   "metadata": {},
   "source": [
    "## Unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4514832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(unet_ckpt.keys()):\n",
    "#     if k.split('.')[-2] in ['time_emb_proj', 'time_emb']:\n",
    "#         unet_ckpt.pop(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca6df9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel(cross_attention_dim=768)\n",
    "unet_ckpt = torch.load('ckpt/unet_model.bin')   \n",
    "unet.load_state_dict(unet_ckpt, strict=False)\n",
    "\n",
    "inp = torch.randn(1,3,64,64)\n",
    "tx_state = torch.randn(1,77,768)\n",
    "\n",
    "opt = unet(inp, tx_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ee5eece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090011",
   "metadata": {},
   "source": [
    "## Train-Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cefbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel_yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "\n",
    "# from models.model_vqa import ALBEF\n",
    "# from models.vit import interpolate_pos_embed\n",
    "# from models.tokenization_bert import BertTokenizer\n",
    "\n",
    "\n",
    "# from dataset.utils import save_result\n",
    "# from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn\n",
    "\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "## tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac404a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config, Writer):\n",
    "\n",
    "    # train\n",
    "    model.train()  \n",
    "    \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('loss', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "\n",
    "    header = 'Train Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 50    \n",
    "    step_size = 100\n",
    "    warmup_iterations = warmup_steps*step_size  \n",
    "    \n",
    "    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image, weights = image.to(device,non_blocking=True), weights.to(device,non_blocking=True)      \n",
    "        question_input = tokenizer(question, padding='longest', truncation=True, max_length=25, return_tensors=\"pt\").to(device) \n",
    "        answer_input = tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device) \n",
    "        \n",
    "        if epoch>0 or not config['warm_up']:\n",
    "            alpha = config['alpha']\n",
    "        else:\n",
    "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
    "\n",
    "        loss = model(image, question_input, answer_input, train=True, alpha=alpha, k=n, weights=weights)        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        \n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "     \n",
    "        if epoch==0 and i%step_size==0 and i<=warmup_iterations: \n",
    "            scheduler.step(i//step_size) \n",
    "            \n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger.global_avg())   \n",
    "    ## Extra added\n",
    "    if dist.get_rank() == 0:\n",
    "        Writer.add_scalar('LOSS', getattr(metric_logger, 'loss'), i+epoch)\n",
    "        Writer.add_scalar('lr', optimizer.param_groups[0]['lr'], i+epoch)\n",
    "        \n",
    "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()} \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluation(model, data_loader, tokenizer, device, config) :\n",
    "    # test\n",
    "    model.eval()\n",
    "            \n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Generate VQA test result:'\n",
    "    print_freq = 50\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    answer_list = [answer+config['eos'] for answer in data_loader.dataset.answer_list]\n",
    "    answer_input = tokenizer(answer_list, padding='longest', return_tensors='pt').to(device)    \n",
    "        \n",
    "    for n, (image, question, question_id) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):        \n",
    "        image = image.to(device,non_blocking=True)             \n",
    "        question_input = tokenizer(question, padding='longest', return_tensors=\"pt\").to(device)        \n",
    "\n",
    "        topk_ids, topk_probs = model(image, question_input, answer_input, train=False, k=config['k_test'])      \n",
    "        \n",
    "        for ques_id, topk_id, topk_prob in zip(question_id, topk_ids, topk_probs):\n",
    "            ques_id = int(ques_id.item())          \n",
    "            _, pred = topk_prob.max(dim=0)\n",
    "            result.append({\"question_id\":ques_id, \"answer\":data_loader.dataset.answer_list[topk_id[pred]]})   \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main(args, config):\n",
    "    ## distribued_init\n",
    "    utils.init_distributed_mode(args)    \n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "    ## Writer\n",
    "    Writer = None\n",
    "    if dist.get_rank ==0 :\n",
    "        Writer = SummaryWriter(config['tensorboard_dir'])\n",
    "        \n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    start_epoch = 0\n",
    "    max_epoch = config['schedular']['epochs']\n",
    "    warmup_steps = config['schedular']['warmup_epochs']\n",
    "    \n",
    "    \n",
    "    #### Dataset #### \n",
    "    print(\"Creating vqa datasets\")\n",
    "    datasets = create_dataset('vqa', config)   \n",
    "    \n",
    "    if args.distributed:\n",
    "        num_tasks = utils.get_world_size()\n",
    "        global_rank = utils.get_rank()            \n",
    "        samplers = create_sampler(datasets, [True, False], num_tasks, global_rank)         \n",
    "    else:\n",
    "        samplers = [None, None]\n",
    "    \n",
    "    train_loader, test_loader = create_loader(datasets,samplers,\n",
    "                                              batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
    "                                              num_workers=[4,4],is_trains=[True, False], \n",
    "                                              collate_fns=[vqa_collate_fn,None]) \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
    "\n",
    "    #### Model #### \n",
    "    print(\"Creating model\")\n",
    "    model = ALBEF(config=config, text_encoder=args.text_encoder, text_decoder=args.text_decoder, tokenizer=tokenizer)\n",
    "    model = model.to(device)   \n",
    "    \n",
    "    arg_opt = utils.AttrDict(config['optimizer'])\n",
    "    optimizer = create_optimizer(arg_opt, model)\n",
    "    arg_sche = utils.AttrDict(config['schedular'])\n",
    "    lr_scheduler, _ = create_scheduler(arg_sche, optimizer)          \n",
    "        \n",
    "    if args.checkpoint:    \n",
    "        checkpoint = torch.load(args.checkpoint, map_location='cpu') \n",
    "        state_dict = checkpoint['model']\n",
    "        \n",
    "        # reshape positional embedding to accomodate for image resolution change\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped   \n",
    "        \n",
    "        if not args.evaluate:\n",
    "            if config['distill']:\n",
    "                m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "                state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "                \n",
    "            for key in list(state_dict.keys()):\n",
    "                if 'bert' in key:\n",
    "                    encoder_key = key.replace('bert.','')         \n",
    "                    state_dict[encoder_key] = state_dict[key] \n",
    "                # intialize text decoder as multimodal encoder (last 6 layers of model.text_encoder)    \n",
    "                if 'text_encoder' in key:                \n",
    "                    if 'layer' in key:\n",
    "                        encoder_keys = key.split('.')\n",
    "                        layer_num = int(encoder_keys[4])\n",
    "                        if layer_num<6:\n",
    "                            del state_dict[key]  \n",
    "                            continue\n",
    "                        else:\n",
    "                            decoder_layer_num = (layer_num-6)\n",
    "                            encoder_keys[4] = str(decoder_layer_num)\n",
    "                            encoder_key = '.'.join(encoder_keys)     \n",
    "                    else:\n",
    "                        encoder_key = key\n",
    "                    decoder_key = encoder_key.replace('text_encoder','text_decoder')  \n",
    "                    state_dict[decoder_key] = state_dict[key]     \n",
    "\n",
    "                    del state_dict[key]                \n",
    "                \n",
    "        msg = model.load_state_dict(state_dict,strict=False)  \n",
    "        print('load checkpoint from %s'%args.checkpoint)\n",
    "        print(msg)  \n",
    "\n",
    "        \n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module    \n",
    "    \n",
    "    \n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        if epoch>0:\n",
    "            lr_scheduler.step(epoch+warmup_steps)  \n",
    "        \n",
    "        if not args.evaluate:\n",
    "            if args.distributed:\n",
    "                train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config, Writer)  ## Add Writer\n",
    "\n",
    "        if args.evaluate:\n",
    "            break\n",
    "            \n",
    "        if utils.is_main_process():               \n",
    "            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                         'epoch': epoch,\n",
    "                        }                \n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")                        \n",
    "                         \n",
    "            save_obj = {\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'config': config,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_%02d.pth'%epoch))  \n",
    "\n",
    "        dist.barrier()   \n",
    "  \n",
    "    vqa_result = evaluation(model, test_loader, tokenizer, device, config)        \n",
    "    result_file = save_result(vqa_result, args.result_dir, 'vqa_result_epoch%d'%epoch)\n",
    "                     \n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str)) \n",
    "    \n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config', default='./configs/VQA.yaml') \n",
    "    parser.add_argument('--checkpoint', default='') \n",
    "    parser.add_argument('--output_dir', default='output/vqa')\n",
    "    parser.add_argument('--evaluate', action='store_true')    \n",
    "    parser.add_argument('--text_encoder', default='bert-base-uncased')\n",
    "    parser.add_argument('--text_decoder', default='bert-base-uncased')\n",
    "    parser.add_argument('--device', default='cuda')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    \n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "    parser.add_argument('--distributed', default=True, type=bool)\n",
    "    ## ADD\n",
    "#     parser.add_argument('--tensorboard_dir', type=str, default='./tensenboard/')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
    "\n",
    "    args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))\n",
    "    \n",
    "    main(args, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
